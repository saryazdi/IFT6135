{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from DataPreparation.dataset_preparation import get_binarymnist_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Load the Dataset\n",
    "Set the data directory to the path where the following files exist:  binarized_mnist_train.amat,  binarized_mnist_valid.amat, binarized_mnist_test.amat <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Dataset/BinaryMNIST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  (50000, 1, 28, 28)\n",
      "Val data size:  (10000, 1, 28, 28)\n",
      "Test data size:  (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, X_train_moments = get_binarymnist_dataset(data_dir, normalize=False)\n",
    "mean_img, std_img = X_train_moments\n",
    "print('Train data size: ', X_train.shape)\n",
    "print('Val data size: ', X_val.shape)\n",
    "print('Test data size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = TensorDataset(torch.from_numpy(X_train))\n",
    "loader_train  = DataLoader(X_train_, batch_size=64, shuffle=True)\n",
    "\n",
    "X_val_ = TensorDataset(torch.from_numpy(X_val))\n",
    "loader_val = DataLoader(X_val_, batch_size=64, shuffle=False)\n",
    "\n",
    "X_test_ = TensorDataset(torch.from_numpy(X_test))\n",
    "loader_test = DataLoader(X_test_, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=GPU\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Using device=GPU') if use_cuda else print('Using device=CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae import VAE\n",
    "num_latent = 100\n",
    "model = VAE(num_latent).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Training with GPU ~~~\n",
      "Model has 938.83K trainable parameters.\n",
      "\n",
      "Epoch 1:\n",
      "Train: elbo -186.3591, logpx_z -169.2217, KL 17.1375\n",
      "Validation: elbo -140.3563, logpx_z -117.9124, KL 22.4439\n",
      "-----------\n",
      "Epoch 2:\n",
      "Train: elbo -127.1850, logpx_z -104.7561, KL 22.4289\n",
      "Validation: elbo -118.3235, logpx_z -95.3080, KL 23.0155\n",
      "-----------\n",
      "Epoch 3:\n",
      "Train: elbo -113.5228, logpx_z -90.3365, KL 23.1862\n",
      "Validation: elbo -110.4282, logpx_z -86.9951, KL 23.4331\n",
      "-----------\n",
      "Epoch 4:\n",
      "Train: elbo -108.1992, logpx_z -84.2305, KL 23.9687\n",
      "Validation: elbo -106.7921, logpx_z -82.2773, KL 24.5147\n",
      "-----------\n",
      "Epoch 5:\n",
      "Train: elbo -104.8240, logpx_z -80.3921, KL 24.4319\n",
      "Validation: elbo -104.0739, logpx_z -79.8675, KL 24.2065\n",
      "-----------\n",
      "Epoch 6:\n",
      "Train: elbo -102.5594, logpx_z -77.7199, KL 24.8395\n",
      "Validation: elbo -102.1658, logpx_z -77.1786, KL 24.9872\n",
      "-----------\n",
      "Epoch 7:\n",
      "Train: elbo -100.8396, logpx_z -75.7237, KL 25.1159\n",
      "Validation: elbo -100.6385, logpx_z -75.6920, KL 24.9465\n",
      "-----------\n",
      "Epoch 8:\n",
      "Train: elbo -99.5574, logpx_z -74.2242, KL 25.3332\n",
      "Validation: elbo -100.0090, logpx_z -74.7373, KL 25.2717\n",
      "-----------\n",
      "Epoch 9:\n",
      "Train: elbo -98.6307, logpx_z -73.1864, KL 25.4443\n",
      "Validation: elbo -98.3265, logpx_z -72.6537, KL 25.6728\n",
      "-----------\n",
      "Epoch 10:\n",
      "Train: elbo -97.8022, logpx_z -72.2822, KL 25.5200\n",
      "Validation: elbo -98.7732, logpx_z -72.7918, KL 25.9814\n",
      "-----------\n",
      "Epoch 11:\n",
      "Train: elbo -97.2000, logpx_z -71.5914, KL 25.6086\n",
      "Validation: elbo -97.8511, logpx_z -72.0564, KL 25.7947\n",
      "-----------\n",
      "Epoch 12:\n",
      "Train: elbo -96.6799, logpx_z -70.9974, KL 25.6825\n",
      "Validation: elbo -96.4660, logpx_z -70.8864, KL 25.5796\n",
      "-----------\n",
      "Epoch 13:\n",
      "Train: elbo -96.0924, logpx_z -70.4380, KL 25.6545\n",
      "Validation: elbo -96.2487, logpx_z -70.5944, KL 25.6543\n",
      "-----------\n",
      "Epoch 14:\n",
      "Train: elbo -95.7521, logpx_z -70.0322, KL 25.7198\n",
      "Validation: elbo -96.1312, logpx_z -70.3957, KL 25.7355\n",
      "-----------\n",
      "Epoch 15:\n",
      "Train: elbo -95.4095, logpx_z -69.6624, KL 25.7471\n",
      "Validation: elbo -95.9126, logpx_z -69.9286, KL 25.9840\n",
      "-----------\n",
      "Epoch 16:\n",
      "Train: elbo -95.0310, logpx_z -69.2819, KL 25.7491\n",
      "Validation: elbo -95.1616, logpx_z -69.5236, KL 25.6381\n",
      "-----------\n",
      "Epoch 17:\n",
      "Train: elbo -94.6776, logpx_z -68.9277, KL 25.7499\n",
      "Validation: elbo -95.1575, logpx_z -69.1118, KL 26.0458\n",
      "-----------\n",
      "Epoch 18:\n",
      "Train: elbo -94.3956, logpx_z -68.6026, KL 25.7930\n",
      "Validation: elbo -94.9455, logpx_z -69.0867, KL 25.8587\n",
      "-----------\n",
      "Epoch 19:\n",
      "Train: elbo -94.2376, logpx_z -68.4112, KL 25.8264\n",
      "Validation: elbo -94.5331, logpx_z -68.8756, KL 25.6575\n",
      "-----------\n",
      "Epoch 20:\n",
      "Train: elbo -93.9184, logpx_z -68.0668, KL 25.8517\n",
      "Validation: elbo -94.3561, logpx_z -68.2783, KL 26.0778\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "from utils.train_eval_utils import train_model\n",
    "print('~~~ Training with GPU ~~~') if use_cuda else print('~~~ Training with CPU ~~~\\n')\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Model has %.2fK trainable parameters.\\n' % (num_params/1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_history = train_model(model, optimizer, loader_train,\n",
    "                            loader_val, num_epochs,\n",
    "                            device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 32\n",
    "K = 200\n",
    "D = 784\n",
    "X = np.reshape(X_val[:M], (M, D))\n",
    "X = torch.from_numpy(X).to(device=device, dtype=torch.float32)\n",
    "Z = torch.randn(M, K, num_latent) # Z gets \"reparameterized\" in minibatch_importance_sampling() so that: z~q(z|x)\n",
    "Z = Z.to(device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For one minibatch of validation data:\n",
      "(log p(x1), . . . , log p(xM)) estimates of size (M,):\n",
      "\n",
      "tensor([ -95.2595,  -63.9816, -113.0852,  -72.8841,  -84.2797,  -98.1954,\n",
      "        -105.4902,  -70.6400,  -45.5383,  -61.5701,  -95.4371,  -96.1672,\n",
      "        -102.5433,  -91.7915,  -74.7288,  -84.0134,  -54.0331,  -71.6146,\n",
      "        -108.3958,  -89.2090,  -75.1369,  -81.7220,  -99.6269,  -89.2135,\n",
      "        -101.3560,  -51.3683,  -90.8322,  -46.3075, -117.1302,  -86.5186,\n",
      "         -43.3720, -121.1844], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utils.importance_sampling import minibatch_importance_sampling\n",
    "logp = minibatch_importance_sampling(model, X, Z, device)\n",
    "print('For one minibatch of validation data:')\n",
    "print('(log p(x1), . . . , log p(xM)) estimates of size (M,):\\n')\n",
    "print(logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Validation and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.importance_sampling import importance_sampling\n",
    "logp_val = importance_sampling(model, loader_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_test = importance_sampling(model, loader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_eval_utils import evaluation, criterion\n",
    "val_elbo, val_logpx_z, val_kl = evaluation(model, loader_val, criterion, device)\n",
    "test_elbo, test_logpx_z, test_kl = evaluation(model, loader_test, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "(approximated) log-likelihood: -88.9486\n",
      "ELBO: -94.4227\n",
      "~~~~~~~~~~~~~\n",
      "Test:\n",
      "(approximated) log-likelihood: -88.3406\n",
      "ELBO: -93.6553\n"
     ]
    }
   ],
   "source": [
    "print('Validation:')\n",
    "print('(approximated) log-likelihood: %.4f' % (logp_val.cpu().numpy()))\n",
    "print('ELBO: %.4f' % val_elbo.cpu().numpy())\n",
    "print('~~~~~~~~~~~~~')\n",
    "print('Test:')\n",
    "print('(approximated) log-likelihood: %.4f' % (logp_test.cpu().numpy()))\n",
    "print('ELBO: %.4f' % test_elbo.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
